{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEG_CONTRACTIONS = [\n",
    "    (r'aren\\'t', 'are not'),\n",
    "    (r'can\\'t', 'can not'),\n",
    "    (r'couldn\\'t', 'could not'),\n",
    "    (r'daren\\'t', 'dare not'),\n",
    "    (r'didn\\'t', 'did not'),\n",
    "    (r'doesn\\'t', 'does not'),\n",
    "    (r'don\\'t', 'do not'),\n",
    "    (r'isn\\'t', 'is not'),\n",
    "    (r'hasn\\'t', 'has not'),\n",
    "    (r'haven\\'t', 'have not'),\n",
    "    (r'hadn\\'t', 'had not'),\n",
    "    (r'mayn\\'t', 'may not'),\n",
    "    (r'mightn\\'t', 'might not'),\n",
    "    (r'mustn\\'t', 'must not'),\n",
    "    (r'needn\\'t', 'need not'),\n",
    "    (r'oughtn\\'t', 'ought not'),\n",
    "    (r'shan\\'t', 'shall not'),\n",
    "    (r'shouldn\\'t', 'should not'),\n",
    "    (r'wasn\\'t', 'was not'),\n",
    "    (r'weren\\'t', 'were not'),\n",
    "    (r'won\\'t', 'will not'),\n",
    "    (r'wouldn\\'t', 'would not'),\n",
    "    (r'ain\\'t', 'am not') # not only but stopword anyway\n",
    "]\n",
    "\n",
    "BLACKLIST_STOPWORDS = ['over','only','very','not','no']\n",
    "ENGLISH_STOPWORDS = set(stopwords.words('english')) - set(BLACKLIST_STOPWORDS)\n",
    "\n",
    "OTHER_CONTRACTIONS = {\n",
    "    \"'m\": 'am',\n",
    "    \"'ll\": 'will',\n",
    "    \"'s\": 'has', # or 'is' but both are stopwords\n",
    "    \"'d\": 'had'  # or 'would' but both are stopwords\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTrainFile(file):\n",
    "    with open(file,'r') as tsvin:\n",
    "        tsvin = csv.reader(tsvin,delimiter ='\\t')\n",
    "        parsedFile = {\"label\" :[],\"statement\" :[],\"subject\" :[],\"speaker\":[],\"speakerJob\":[],\"stateInfo\":[],\"partyAffiliation\":[],\"context\":[]}\n",
    "        for rowNum,row in enumerate(tsvin):\n",
    "            try:\n",
    "                parsedFile[\"label\"].append(row[0])\n",
    "                parsedFile[\"statement\"].append(row[1])\n",
    "                parsedFile[\"subject\"].append(row[2])\n",
    "                parsedFile[\"speaker\"].append(row[3])\n",
    "                parsedFile[\"speakerJob\"].append(row[4])\n",
    "                parsedFile[\"stateInfo\"].append(row[5])\n",
    "                parsedFile[\"partyAffiliation\"].append(row[6])\n",
    "                parsedFile[\"context\"].append(row[7])\n",
    "            except:\n",
    "                print(\"Few inputs are in invalid format\")\n",
    "                #print(rowNum)\n",
    "                #print(row)\n",
    "\n",
    "        return parsedFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTestFile(file):\n",
    "    with open(file,'r') as tsvin:\n",
    "        tsvin = csv.reader(tsvin,delimiter ='\\t')\n",
    "        parsedFile = {\"statement\" :[],\"subject\" :[],\"speaker\":[],\"speakerJob\":[],\"stateInfo\":[],\"partyAffiliation\":[],\"context\":[]}\n",
    "        for rowNum,row in enumerate(tsvin):\n",
    "            try:\n",
    "                parsedFile[\"statement\"].append(row[0])\n",
    "                parsedFile[\"subject\"].append(row[1])\n",
    "                parsedFile[\"speaker\"].append(row[2])\n",
    "                parsedFile[\"speakerJob\"].append(row[3])\n",
    "                parsedFile[\"stateInfo\"].append(row[4])\n",
    "                parsedFile[\"partyAffiliation\"].append(row[5])\n",
    "                parsedFile[\"context\"].append(row[6])\n",
    "            except:\n",
    "                print(\"Few inputs are in invalid format\")\n",
    "                print(rowNum)\n",
    "                print(row)\n",
    "\n",
    "        return parsedFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input statement is expected a string.\n",
    "def preProcessing(text,delimiter=' ',n=1):\n",
    "    tokenisedOutput = []\n",
    "    stemmer = PorterStemmer()\n",
    "    for line in text:\n",
    "        tokens = []\n",
    "\n",
    "        ## Convert the line into lower case\n",
    "        line = line.lower()\n",
    "\n",
    "        ## Transform negative contractions\n",
    "        for neg in NEG_CONTRACTIONS:\n",
    "            line = re.sub(neg[0], neg[1], line)\n",
    "\n",
    "        ## Tokenising the words\n",
    "        tokens = word_tokenize(line)\n",
    "\n",
    "        # transform other contractions (e.g 'll --> will)\n",
    "        tokens = [OTHER_CONTRACTIONS[token] if OTHER_CONTRACTIONS.get(token)\n",
    "                  else token for token in tokens]\n",
    "\n",
    "        # removing punctuations, only retain words, no numbers and punctuation marks.\n",
    "        r = r'[a-z]+'\n",
    "        tokens = [word for word in tokens if re.search(r, word)]\n",
    "\n",
    "        # # remove irrelevant stop words\n",
    "        # tokens = [token for token in tokens if token not in ENGLISH_STOPWORDS]\n",
    "\n",
    "        # stemming\n",
    "        #tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "\n",
    "        ## Probably not required if using RNN for classification\n",
    "        if n == 1:\n",
    "            # return the list of words\n",
    "            tokenisedOutput.append(tokens)\n",
    "        else:\n",
    "            # return the list of ngrams\n",
    "            tokenisedOutput.append(ngrams(tokens, n))\n",
    "        ##print(tokens)\n",
    "\n",
    "    return tokenisedOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Returns the indice of the statment which can be used for embedding lookup\n",
    "maxSeqLength = 200\n",
    "\n",
    "def statementIndices(text,dictionary,outputLength):\n",
    "    tokenListIndices = np.zeros((outputLength,maxSeqLength))\n",
    "    lineCount = 0\n",
    "    tokenCount = 0\n",
    "    \n",
    "    for line in text:\n",
    "        tokenCount = 0\n",
    "        for token in line:\n",
    "            try:\n",
    "                tokenListIndices[lineCount][tokenCount] = dictionary[token]\n",
    "            except:\n",
    "                tokenListIndices[lineCount][tokenCount] = 399999\n",
    "            tokenCount = tokenCount + 1\n",
    "            if(tokenCount >= maxSeqLength):\n",
    "                break\n",
    "        lineCount = lineCount + 1\n",
    "\n",
    "    return tokenListIndices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelVectors(labels):\n",
    "    labelVectors = []\n",
    "    defaultVectors = {\"pants-fire\":np.array([1,0,0,0,0,0]),\"false\":np.array([0,1,0,0,0,0]),\"barely-true\":np.array([0,0,1,0,0,0]),\n",
    "                      \"half-true\":np.array([0,0,0,1,0,0]),\"mostly-true\":np.array([0,0,0,0,1,0]),\"true\":np.array([0,0,0,0,0,1])}\n",
    "    for label in labels:\n",
    "        labelVectors.append(defaultVectors[label])\n",
    "    return np.asarray(labelVectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadGlove(embeddingFile):\n",
    "    vocab = []\n",
    "    embedding = []\n",
    "    dictionary = {}\n",
    "    reverseDictionary = {}\n",
    "    count = 0\n",
    "    file = open(embeddingFile, 'r')\n",
    "    for line in file.readlines():\n",
    "        row = line.strip().split(' ')\n",
    "        vocab.append(row[0])\n",
    "        embedding.append(row[1:])\n",
    "        dictionary[row[0]] = count\n",
    "        reverseDictionary[count] = row[0]\n",
    "        count = count + 1\n",
    "    print('Loaded GloVe!')\n",
    "    file.close()\n",
    "    return vocab, embedding,dictionary,reverseDictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingFile = \"/Users/sainikhilmaram/OneDrive/UCSB courses/Winter 2018/Deep Learning/HW2/liar_dataset/train.tsv\"\n",
    "embeddingFile = \"/Users/sainikhilmaram/Desktop/glove/glove.6B.300d.txt\"\n",
    "testFile = \"/Users/sainikhilmaram/OneDrive/UCSB courses/Winter 2018/Deep Learning/HW2/liar_dataset/test.tsv\"\n",
    "validFile = \"/Users/sainikhilmaram/OneDrive/UCSB courses/Winter 2018/Deep Learning/HW2/liar_dataset/valid.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded GloVe!\n"
     ]
    }
   ],
   "source": [
    "vocab,embedding,dictionary,reverseDictionary = loadGlove(embeddingFile)\n",
    "vocabSize = len(vocab)\n",
    "embeddingSize = len(embedding[0]) ## 300\n",
    "embedding = np.asarray(embedding)\n",
    "vocab = np.asarray(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few inputs are in invalid format\n",
      "Few inputs are in invalid format\n"
     ]
    }
   ],
   "source": [
    "parsedTraining = readTrainFile(trainingFile)\n",
    "## Tokenising the statement file\n",
    "tokenisedStatement = preProcessing(parsedTraining[\"statement\"])\n",
    "## getting the indices of the word.\n",
    "tokenisedStatementIndices = statementIndices(tokenisedStatement,dictionary,len(tokenisedStatement))\n",
    "## Output labels are converted into vectors\n",
    "outputLabelVectors = labelVectors(parsedTraining[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsedValid = readTrainFile(validFile)\n",
    "tokenisedStatementValid = preProcessing(parsedValid[\"statement\"])\n",
    "tokenisedStatementIndicesValid = statementIndices(tokenisedStatementValid,dictionary,len(tokenisedStatementValid))\n",
    "outputLabelVectorsValid = labelVectors(parsedValid[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsedTest = readTestFile(testFile)\n",
    "tokenisedStatementTest = preProcessing(parsedTest[\"statement\"])\n",
    "## getting the indices of the word.\n",
    "tokenisedStatementIndicesTest = statementIndices(tokenisedStatementTest,dictionary,len(tokenisedStatementTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddingMatrix(sess,vocabSize,embeddingSize,embedding):\n",
    "    W = tf.Variable(tf.constant(0.0, shape=[vocabSize, embeddingSize]),\n",
    "                    trainable=False, name=\"W\")\n",
    "    embeddingPlaceholder = tf.placeholder(tf.float32, shape=[vocabSize, embeddingSize])\n",
    "    embeddingInit = W.assign(embeddingPlaceholder)\n",
    "    sess.run(embeddingInit, feed_dict={embeddingPlaceholder: embedding})\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## returns the embedding matrix which can be used for look up\n",
    "embeddingMatrixWeights = embeddingMatrix(sess,vocabSize,embeddingSize,embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.25539  -0.25723   0.13169  ... -0.2329   -0.12226   0.35499 ]\n",
      " [-0.12559   0.01363   0.10306  ... -0.34224  -0.022394  0.13684 ]\n",
      " [-0.076947 -0.021211  0.21271  ...  0.18351  -0.29183  -0.046533]\n",
      " [-0.25756  -0.057132 -0.6719   ... -0.16043   0.046744 -0.070621]\n",
      " [ 0.038466 -0.039792  0.082747 ... -0.33427   0.011807  0.059703]]\n"
     ]
    }
   ],
   "source": [
    "wordIndices = tf.placeholder(tf.int32,shape=[None])\n",
    "embeddedWords = tf.nn.embedding_lookup(embeddingMatrixWeights,wordIndices)\n",
    "\n",
    "print(sess.run(embeddedWords,feed_dict={wordIndices:[1,2,3,4,5]}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 64\n",
    "lstmUnits = 64\n",
    "numClasses = 6\n",
    "iterations = 10\n",
    "keepProb = 0.95\n",
    "numLayers = 3\n",
    "initialLearningRate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLstmCell(lstmUnits = lstmUnits, keepProb=keepProb):\n",
    "    ## Basic LSTM cell is created\n",
    "    lstmCell = tf.contrib.rnn.BasicLSTMCell(lstmUnits)\n",
    "    ## Dropout  wrapper is created\n",
    "    lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=keep_prob)\n",
    "    return lstmCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_data,embeddingMatrixWeights):\n",
    "    ## embedding loookup for the input data from the embedding matrix\n",
    "    data = tf.nn.embedding_lookup(embeddingMatrixWeights,input_data)\n",
    "    \n",
    "    ## Multi layer RNN\n",
    "    lstmCell = tf.nn.rnn_cell.MultiRNNCell([getLstmCell(lstmUnits, keepProb) for _ in range(numLayers)], state_is_tuple=True)\n",
    "    \n",
    "    ## Dynamic rolling of LSTM\n",
    "    value, _ = tf.nn.dynamic_rnn(lstmCell, data, dtype=tf.float32)\n",
    "    ## Each cell will give us a output\n",
    "    value = tf.transpose(value, [1, 0, 2])\n",
    "    ## Only last layer is considered\n",
    "    last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "    ## Weights for the last layer to get a 6 dimensional vector\n",
    "    weight = tf.Variable(tf.truncated_normal([lstmUnits, numClasses]))\n",
    "    ## Bias for each class\n",
    "    bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))\n",
    "    ## Prediction\n",
    "    prediction = (tf.matmul(last, weight) + bias)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainBatch(index,batchSize,tokenisedStatementIndices,outputLabelVectors):\n",
    "    return tokenisedStatementIndices[index : index + batchSize] , np.array(outputLabelVectors[index:index+batchSize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTestBatch(index,batchSize,tokenisedStatementIndicesTest):\n",
    "    return tokenisedStatementIndicesTest[index : index + batchSize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(tokenisedStatementIndices,outputLabelVectors,vocabSize,embeddingSize,embedding,tokenisedStatementIndicesValid,\n",
    "               outputLabelVectorsValid,iterations=10):\n",
    "    \n",
    "    trainingDataSize = len(tokenisedStatementIndices)\n",
    "    prevAccuracy = 0\n",
    "    with tf.Session() as sess:\n",
    "        embeddingMatrixWeights = embeddingMatrix(sess,vocabSize,embeddingSize,embedding)\n",
    "        labels = tf.placeholder(tf.float32, [None, numClasses])\n",
    "        input_data = tf.placeholder(tf.int32, [None, maxSeqLength])\n",
    "        prediction = build_model(input_data,embeddingMatrixWeights)\n",
    "        correctPred = tf.equal(tf.argmax(prediction,1), tf.argmax(labels,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))\n",
    "        \n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=labels))\n",
    "        optimizer = tf.train.AdamOptimizer(initialLearningRate).minimize(loss)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for i in range(iterations):\n",
    "            currAccuracy = sess.run(accuracy,feed_dict={input_data:tokenisedStatementIndicesValid,labels:outputLabelVectorsValid})\n",
    "            print(\"Current Accuracy : \", currAccuracy)\n",
    "            if(currAccuracy + 0.1 > prevAccuracy):\n",
    "                prevAccuracy = currAccuracy\n",
    "                index = 0\n",
    "                ## If data present is not exact multiple of batch size\n",
    "                while index < trainingDataSize:\n",
    "                    if(index + batchSize <= trainingDataSize):\n",
    "                        size = batchSize\n",
    "                    else:\n",
    "                        size = trainingDataSize - index\n",
    "                    inputData,outputData = getTrainBatch(index,size,tokenisedStatementIndices,outputLabelVectors)\n",
    "                    sess.run(optimizer,feed_dict={input_data:inputData,labels:outputData})\n",
    "                    index = index + size\n",
    "                saver = tf.train.Saver()\n",
    "                save_path = saver.save(sess, \"//Users/sainikhilmaram/OneDrive/UCSB courses/Winter 2018/Deep Learning/HW2/liar_dataset/model/model.ckpt\")\n",
    "                \n",
    "            else:\n",
    "                print(\"Saturation already achieved in previous iteration  : \",i)\n",
    "                break\n",
    "        print(\"Model saved in path: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:97: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Accuracy :  0.09034268\n",
      "Current Accuracy :  0.19548286\n",
      "Current Accuracy :  0.19080997\n",
      "Current Accuracy :  0.19158879\n",
      "Current Accuracy :  0.1970405\n",
      "Current Accuracy :  0.19470406\n",
      "Current Accuracy :  0.19470406\n",
      "Current Accuracy :  0.1923676\n",
      "Current Accuracy :  0.19470406\n",
      "Current Accuracy :  0.19314642\n",
      "Model saved in path: //Users/sainikhilmaram/OneDrive/UCSB courses/Winter 2018/Deep Learning/HW2/liar_dataset/model/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "trainModel(tokenisedStatementIndices,outputLabelVectors,vocabSize,embeddingSize,\n",
    "           embedding,tokenisedStatementIndicesValid,outputLabelVectorsValid,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testModel(tokenisedStatementIndicesTest,vocabSize,embeddingSize,embedding):\n",
    "    outputPrediction = []\n",
    "    testDataSize = len(tokenisedStatementIndicesTest)\n",
    "    with tf.Session() as sess:\n",
    "        embeddingMatrixWeights = embeddingMatrix(sess,vocabSize,embeddingSize,embedding)\n",
    "        labels = tf.placeholder(tf.float32, [None, numClasses])\n",
    "        input_data = tf.placeholder(tf.int32, [None, maxSeqLength])\n",
    "        prediction = build_model(input_data,embeddingMatrixWeights)\n",
    "        correctPrediction = tf.argmax(prediction,1)\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess,\"//Users/sainikhilmaram/OneDrive/UCSB courses/Winter 2018/Deep Learning/HW2/liar_dataset/model/model.ckpt\")\n",
    "        index = 0\n",
    "        while index < testDataSize:\n",
    "            if(index + batchSize <= testDataSize):\n",
    "                size = batchSize\n",
    "            else:\n",
    "                size = testDataSize - index\n",
    "            inputData = getTestBatch(index,batchSize,tokenisedStatementIndicesTest)\n",
    "            outputPrediction.extend(sess.run(correctPrediction,feed_dict={input_data:inputData}))\n",
    "            index = index + size\n",
    "    return outputPrediction\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from //Users/sainikhilmaram/OneDrive/UCSB courses/Winter 2018/Deep Learning/HW2/liar_dataset/model/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "outputPrediction = testModel(tokenisedStatementIndicesTest,vocabSize,embeddingSize,embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputLabels = {0:\"pants-fire\",1:\"false\",2:\"barely-true\",3:\"half-true\",4:\"mostly-true\",5:\"true\"}\n",
    "def saveFile(outputPrediction,fileName):\n",
    "    f = open(fileName,'w')\n",
    "    for i in range(len(outputPrediction)):\n",
    "        s = inputLabels[outputPrediction[i]]\n",
    "        s = s +\"\\n\"\n",
    "        f.write(s)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveFile(outputPrediction,\"predictions.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
