{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEG_CONTRACTIONS = [\n",
    "    (r'aren\\'t', 'are not'),\n",
    "    (r'can\\'t', 'can not'),\n",
    "    (r'couldn\\'t', 'could not'),\n",
    "    (r'daren\\'t', 'dare not'),\n",
    "    (r'didn\\'t', 'did not'),\n",
    "    (r'doesn\\'t', 'does not'),\n",
    "    (r'don\\'t', 'do not'),\n",
    "    (r'isn\\'t', 'is not'),\n",
    "    (r'hasn\\'t', 'has not'),\n",
    "    (r'haven\\'t', 'have not'),\n",
    "    (r'hadn\\'t', 'had not'),\n",
    "    (r'mayn\\'t', 'may not'),\n",
    "    (r'mightn\\'t', 'might not'),\n",
    "    (r'mustn\\'t', 'must not'),\n",
    "    (r'needn\\'t', 'need not'),\n",
    "    (r'oughtn\\'t', 'ought not'),\n",
    "    (r'shan\\'t', 'shall not'),\n",
    "    (r'shouldn\\'t', 'should not'),\n",
    "    (r'wasn\\'t', 'was not'),\n",
    "    (r'weren\\'t', 'were not'),\n",
    "    (r'won\\'t', 'will not'),\n",
    "    (r'wouldn\\'t', 'would not'),\n",
    "    (r'ain\\'t', 'am not') # not only but stopword anyway\n",
    "]\n",
    "\n",
    "BLACKLIST_STOPWORDS = ['over','only','very','not','no']\n",
    "ENGLISH_STOPWORDS = set(stopwords.words('english')) - set(BLACKLIST_STOPWORDS)\n",
    "\n",
    "OTHER_CONTRACTIONS = {\n",
    "    \"'m\": 'am',\n",
    "    \"'ll\": 'will',\n",
    "    \"'s\": 'has', # or 'is' but both are stopwords\n",
    "    \"'d\": 'had'  # or 'would' but both are stopwords\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTrainFile(file):\n",
    "    with open(file,'r') as tsvin:\n",
    "        tsvin = csv.reader(tsvin,delimiter ='\\t')\n",
    "        parsedFile = {\"label\" :[],\"statement\" :[],\"subject\" :[],\"speaker\":[],\"speakerJob\":[],\"stateInfo\":[],\"partyAffiliation\":[],\"context\":[]}\n",
    "        for rowNum,row in enumerate(tsvin):\n",
    "            try:\n",
    "                parsedFile[\"label\"].append(row[0])\n",
    "                parsedFile[\"statement\"].append(row[1])\n",
    "                parsedFile[\"subject\"].append(row[2])\n",
    "                parsedFile[\"speaker\"].append(row[3])\n",
    "                parsedFile[\"speakerJob\"].append(row[4])\n",
    "                parsedFile[\"stateInfo\"].append(row[5])\n",
    "                parsedFile[\"partyAffiliation\"].append(row[6])\n",
    "                parsedFile[\"context\"].append(row[7])\n",
    "            except:\n",
    "                print(\"Few inputs are in invalid format\")\n",
    "                #print(rowNum)\n",
    "                #print(row)\n",
    "\n",
    "        return parsedFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTestFile(file):\n",
    "    with open(file,'r') as tsvin:\n",
    "        tsvin = csv.reader(tsvin,delimiter ='\\t')\n",
    "        parsedFile = {\"statement\" :[],\"subject\" :[],\"speaker\":[],\"speakerJob\":[],\"stateInfo\":[],\"partyAffiliation\":[],\"context\":[]}\n",
    "        for rowNum,row in enumerate(tsvin):\n",
    "            try:\n",
    "                parsedFile[\"statement\"].append(row[0])\n",
    "                parsedFile[\"subject\"].append(row[1])\n",
    "                parsedFile[\"speaker\"].append(row[2])\n",
    "                parsedFile[\"speakerJob\"].append(row[3])\n",
    "                parsedFile[\"stateInfo\"].append(row[4])\n",
    "                parsedFile[\"partyAffiliation\"].append(row[5])\n",
    "                parsedFile[\"context\"].append(row[6])\n",
    "            except:\n",
    "                print(\"Few inputs are in invalid format\")\n",
    "                print(rowNum)\n",
    "                print(row)\n",
    "\n",
    "        return parsedFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1283\n"
     ]
    }
   ],
   "source": [
    "print(len(parsedTest[\"statement\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input statement is expected a string.\n",
    "def preProcessing(text,delimiter=' ',n=1):\n",
    "    tokenisedOutput = []\n",
    "    stemmer = PorterStemmer()\n",
    "    for line in text:\n",
    "        tokens = []\n",
    "\n",
    "        ## Convert the line into lower case\n",
    "        line = line.lower()\n",
    "\n",
    "        ## Transform negative contractions\n",
    "        for neg in NEG_CONTRACTIONS:\n",
    "            line = re.sub(neg[0], neg[1], line)\n",
    "\n",
    "        ## Tokenising the words\n",
    "        tokens = word_tokenize(line)\n",
    "\n",
    "        # transform other contractions (e.g 'll --> will)\n",
    "        tokens = [OTHER_CONTRACTIONS[token] if OTHER_CONTRACTIONS.get(token)\n",
    "                  else token for token in tokens]\n",
    "\n",
    "        # removing punctuations, only retain words, no numbers and punctuation marks.\n",
    "        r = r'[a-z]+'\n",
    "        tokens = [word for word in tokens if re.search(r, word)]\n",
    "\n",
    "        # # remove irrelevant stop words\n",
    "        # tokens = [token for token in tokens if token not in ENGLISH_STOPWORDS]\n",
    "\n",
    "        # stemming\n",
    "        #tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "\n",
    "        ## Probably not required if using RNN for classification\n",
    "        if n == 1:\n",
    "            # return the list of words\n",
    "            tokenisedOutput.append(tokens)\n",
    "        else:\n",
    "            # return the list of ngrams\n",
    "            tokenisedOutput.append(ngrams(tokens, n))\n",
    "        ##print(tokens)\n",
    "\n",
    "    return tokenisedOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Returns the indice of the statment which can be used for embedding lookup\n",
    "maxSeqLength = 200\n",
    "\n",
    "def statementIndices(text,dictionary,outputLength):\n",
    "    tokenListIndices = np.zeros((outputLength,maxSeqLength))\n",
    "    lineCount = 0\n",
    "    tokenCount = 0\n",
    "    \n",
    "    for line in text:\n",
    "        tokenCount = 0\n",
    "        for token in line:\n",
    "            try:\n",
    "                tokenListIndices[lineCount][tokenCount] = dictionary[token]\n",
    "            except:\n",
    "                tokenListIndices[lineCount][tokenCount] = 399999\n",
    "            tokenCount = tokenCount + 1\n",
    "            if(tokenCount >= maxSeqLength):\n",
    "                break\n",
    "        lineCount = lineCount + 1\n",
    "\n",
    "    return tokenListIndices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelVectors(labels):\n",
    "    labelVectors = []\n",
    "    defaultVectors = {\"pants-fire\":np.array([1,0,0,0,0,0]),\"false\":np.array([0,1,0,0,0,0]),\"barely-true\":np.array([0,0,1,0,0,0]),\n",
    "                      \"half-true\":np.array([0,0,0,1,0,0]),\"mostly-true\":np.array([0,0,0,0,1,0]),\"true\":np.array([0,0,0,0,0,1])}\n",
    "    for label in labels:\n",
    "        labelVectors.append(defaultVectors[label])\n",
    "    return np.asarray(labelVectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadGlove(embeddingFile):\n",
    "    vocab = []\n",
    "    embedding = []\n",
    "    dictionary = {}\n",
    "    reverseDictionary = {}\n",
    "    count = 0\n",
    "    file = open(embeddingFile, 'r')\n",
    "    for line in file.readlines():\n",
    "        row = line.strip().split(' ')\n",
    "        vocab.append(row[0])\n",
    "        embedding.append(row[1:])\n",
    "        dictionary[row[0]] = count\n",
    "        reverseDictionary[count] = row[0]\n",
    "        count = count + 1\n",
    "    print('Loaded GloVe!')\n",
    "    file.close()\n",
    "    return vocab, embedding,dictionary,reverseDictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingFile = \"/Users/sainikhilmaram/OneDrive/UCSB courses/Winter 2018/Deep Learning/HW2/liar_dataset/train.tsv\"\n",
    "embeddingFile = \"/Users/sainikhilmaram/Desktop/glove/glove.6B.300d.txt\"\n",
    "testFile = \"/Users/sainikhilmaram/OneDrive/UCSB courses/Winter 2018/Deep Learning/HW2/liar_dataset/test.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded GloVe!\n"
     ]
    }
   ],
   "source": [
    "vocab,embedding,dictionary,reverseDictionary = loadGlove(embeddingFile)\n",
    "vocabSize = len(vocab)\n",
    "embeddingSize = len(embedding[0]) ## 300\n",
    "embedding = np.asarray(embedding)\n",
    "vocab = np.asarray(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few inputs are in invalid format\n",
      "Few inputs are in invalid format\n",
      "10240\n"
     ]
    }
   ],
   "source": [
    "parsedTraining = readTrainFile(trainingFile)\n",
    "## Tokenising the statement file\n",
    "##tokenisedStatement = preProcessing([\"I shouldn't,have came here at 3\",\"I'll be the Boss.\"])\n",
    "tokenisedStatement = preProcessing(parsedTraining[\"statement\"])\n",
    "\n",
    "## getting the indices of the word.\n",
    "tokenisedStatementIndices = statementIndices(tokenisedStatement,dictionary,len(tokenisedStatement))\n",
    "#print(tokenisedStatementIndices[0])\n",
    "\n",
    "## Output labels are converted into vectors\n",
    "outputLabelVectors = labelVectors(parsedTraining[\"label\"])\n",
    "\n",
    "print(len(outputLabelVectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10240\n"
     ]
    }
   ],
   "source": [
    "print(len(parsedTraining[\"statement\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsedTest = readTestFile(testFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1283\n"
     ]
    }
   ],
   "source": [
    "print(len(parsedTest[\"statement\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsedTest = readTestFile(testFile)\n",
    "tokenisedStatementTest = preProcessing(parsedTest[\"statement\"])\n",
    "## getting the indices of the word.\n",
    "tokenisedStatementIndicesTest = statementIndices(tokenisedStatementTest,dictionary,len(tokenisedStatementTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.10000e+02 0.00000e+00 2.65469e+05 ... 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00]\n",
      " [6.10000e+01 1.19000e+02 0.00000e+00 ... 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00]\n",
      " [4.53900e+03 4.43000e+02 6.14300e+03 ... 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00]\n",
      " ...\n",
      " [4.10000e+01 9.13000e+02 0.00000e+00 ... 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00]\n",
      " [2.12000e+02 2.00000e+01 2.47000e+02 ... 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00]\n",
      " [2.10000e+02 3.52100e+03 1.41700e+03 ... 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(tokenisedStatementIndices[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0 0 0]\n",
      " [0 0 0 1 0 0]\n",
      " [0 0 0 0 1 0]\n",
      " [0 1 0 0 0 0]\n",
      " [0 0 0 1 0 0]\n",
      " [0 0 0 0 0 1]\n",
      " [0 0 1 0 0 0]\n",
      " [0 0 0 1 0 0]\n",
      " [0 0 0 1 0 0]\n",
      " [0 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(outputLabelVectors[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n",
      "300\n",
      "-0.08155\n"
     ]
    }
   ],
   "source": [
    "print(vocabSize)\n",
    "print(embeddingSize)\n",
    "print(embedding[4736][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddingMatrix(sess,vocabSize,embeddingSize,embedding):\n",
    "    W = tf.Variable(tf.constant(0.0, shape=[vocabSize, embeddingSize]),\n",
    "                    trainable=False, name=\"W\")\n",
    "    embeddingPlaceholder = tf.placeholder(tf.float32, shape=[vocabSize, embeddingSize])\n",
    "    embeddingInit = W.assign(embeddingPlaceholder)\n",
    "    sess.run(embeddingInit, feed_dict={embeddingPlaceholder: embedding})\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## returns the embedding matrix which can be used for look up\n",
    "embeddingMatrixWeights = embeddingMatrix(sess,vocabSize,embeddingSize,embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.25539  -0.25723   0.13169  ... -0.2329   -0.12226   0.35499 ]\n",
      " [-0.12559   0.01363   0.10306  ... -0.34224  -0.022394  0.13684 ]\n",
      " [-0.076947 -0.021211  0.21271  ...  0.18351  -0.29183  -0.046533]\n",
      " [-0.25756  -0.057132 -0.6719   ... -0.16043   0.046744 -0.070621]\n",
      " [ 0.038466 -0.039792  0.082747 ... -0.33427   0.011807  0.059703]]\n"
     ]
    }
   ],
   "source": [
    "wordIndices = tf.placeholder(tf.int32,shape=[None])\n",
    "embeddedWords = tf.nn.embedding_lookup(embeddingMatrixWeights,wordIndices)\n",
    "\n",
    "print(sess.run(embeddedWords,feed_dict={wordIndices:[1,2,3,4,5]}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 24\n",
    "lstmUnits = 64\n",
    "numClasses = 6\n",
    "iterations = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Place holders for input data and labels\n",
    "labels = tf.placeholder(tf.float32, [None, numClasses])\n",
    "input_data = tf.placeholder(tf.int32, [None, maxSeqLength])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = tf.Variable(tf.zeros([batchSize, None, embeddingSize]),dtype=tf.float32)\n",
    "data = tf.nn.embedding_lookup(embeddingMatrixWeights,input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstmCell = tf.contrib.rnn.BasicLSTMCell(lstmUnits)\n",
    "lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)\n",
    "value, _ = tf.nn.dynamic_rnn(lstmCell, data, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = tf.Variable(tf.truncated_normal([lstmUnits, numClasses]))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))\n",
    "value = tf.transpose(value, [1, 0, 2])\n",
    "last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "prediction = (tf.matmul(last, weight) + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctPred = tf.equal(tf.argmax(prediction,1), tf.argmax(labels,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-26-773132a4e1b5>:1: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:97: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=labels))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainBatch(index,batchSize,tokenisedStatementIndices,outputLabelVectors):\n",
    "    return tokenisedStatementIndices[index : index + batchSize] , np.array(outputLabelVectors[index:index+batchSize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTestBatch(index,batchSize,tokenisedStatementIndicesTest):\n",
    "    return tokenisedStatementIndicesTest[index : index + batchSize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(sess,tokenisedStatementIndices,outputLabelVectors,iterations=10):\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    trainingDataSize = len(tokenisedStatementIndices)\n",
    "    for i in range(iterations):\n",
    "        index = 0\n",
    "        ## If data present is not exact multiple of batch size\n",
    "        while index < trainingDataSize:\n",
    "            if(index + batchSize <= trainingDataSize):\n",
    "                size = batchSize\n",
    "            else:\n",
    "                size = trainingDataSize - index\n",
    "            inputData,outputData = getTrainBatch(index,size,tokenisedStatementIndices,outputLabelVectors)\n",
    "            sess.run(optimizer,feed_dict={input_data:inputData,labels:outputData})\n",
    "            index = index + size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainModel(sess,tokenisedStatementIndices,outputLabelVectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sess.run(tf.global_variables_initializer())\n",
    "# trainingDataSize = len(tokenisedStatementIndices)\n",
    "# for i in range(1):\n",
    "#     index = 0\n",
    "#     ## If data present is not exact multiple of batch size\n",
    "#     while index < trainingDataSize:\n",
    "#         if(index + batchSize <= trainingDataSize):\n",
    "#             size = batchSize\n",
    "#         else:\n",
    "#             size = trainingDataSize - index\n",
    "#         inputData,outputData = getTrainBatch(index,size,tokenisedStatementIndices,outputLabelVectors)\n",
    "#         sess.run(optimizer,feed_dict={input_data:inputData,labels:outputData})\n",
    "#         index = index + size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctPrediction = tf.argmax(prediction,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputPrediction = []\n",
    "testDataSize = len(tokenisedStatementIndicesTest)\n",
    "for i in range(1):\n",
    "    index = 0\n",
    "    while index < testDataSize:\n",
    "        if(index + batchSize <= testDataSize):\n",
    "            size = batchSize\n",
    "        else:\n",
    "            size = testDataSize - index\n",
    "        inputData = getTestBatch(index,batchSize,tokenisedStatementIndicesTest)\n",
    "        outputPrediction.extend(sess.run(correctPrediction,feed_dict={input_data:inputData}))\n",
    "        index = index + size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1283\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenisedStatementIndicesTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1283\n"
     ]
    }
   ],
   "source": [
    "print(len(outputPrediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputLabels = {1:\"pants-fire\",2:\"false\",3:\"barely-true\",4:\"half-true\",5:\"mostly-true\",6:\"true\"}\n",
    "def saveFile(outputPrediction,fileName):\n",
    "    f = open(fileName,'w')\n",
    "    for i in range(len(outputPrediction)):\n",
    "        s = inputLabels[outputPrediction[i]]\n",
    "        s = s +\"\\n\"\n",
    "        f.write(s)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveFile(outputPrediction,\"predictions.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
